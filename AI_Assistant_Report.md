# ğŸ’¬ Open Assistant â€” Open-Source AI Chatbot

An open-source AI assistant powered by Qwen2.5 running locally via [Ollama](https://ollama.com), with a beautiful Streamlit UI, system prompt transparency, offline operation, and multilingual readiness (default: English).

---

## ğŸ“ Folder Structure

```bash
open_assistant_project/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ prompt_config.py
â”‚   â”œâ”€â”€ ollama_api.py
â”‚   â”œâ”€â”€ feedback_storage.json
â”‚   â”œâ”€â”€ file_processors/
â”‚   â”‚   â”œâ”€â”€ transcriber.py
â”‚   â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”‚   â”œâ”€â”€ media_analyzer.py
â”‚   â”‚   â””â”€â”€ drive_handler.py
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ logo.png  # (optional for branding)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ report.md

---

## ğŸš€ Features
ğŸ§  Powered by Qwen2.5 via Ollama

ğŸ’» 100% Offline capable

âœ¨ Modern, smooth Streamlit UI

ğŸ—£ï¸ Indic language ready (default: English)

ğŸ” Persistent chat memory

ğŸ“¥ Upload any file (audio, video, PDF, etc.)

ğŸ§ Audio transcription + summarization

ğŸ“Š Integrated user feedback system

ğŸ” Transparent system prompt config

ğŸ“‚ Google Drive file fetch support

ğŸ“ˆ Feedback Analytics Page (WIP)

---

## ğŸ§‘â€ğŸ’» Installation
```bash
# 1. Clone project
git clone https://github.com/yourusername/open_assistant_project
cd open_assistant_project

# 2. Create virtual environment
python -m venv venv

# 3. Activate environment
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate

# 4. Install all dependencies
pip install -r requirements.txt
```

## ğŸ§  Setup Ollama + Qwen2.5
```bash
# Install Ollama from https://ollama.com/download
# Then pull the model:
ollama pull qwen2.5
```

## â–¶ï¸ Run the Open Assistant
```bash
streamlit run app/main.py
```
## ğŸ’¬ System Prompt Transparency
```python
# In app/prompt_config.py
SYSTEM_PROMPT = """
You are an open-source AI assistant that communicates clearly...
"""
```

## ğŸ“ˆ Potential Extensions
ğŸ—ƒï¸ Full Feedback Analytics Page (in development)
ğŸŒ Private LLM switcher (LLaMA 3, Phi-3, Mistral)
ğŸ—£ï¸ Voice-to-Voice Chat Loop (Coqui + Whisper)
ğŸ§© Plugin System for Task Execution

---
# ğŸ™‹ Why This Project?
This project addresses a growing need for private, offline AI tools that can still offer robust functionality across:
-Education (local learning assistant)
-Research (summarize papers, extract audio insights)
-Accessibility (speech-based input/output)
-Regional Language Support (Indic languages)

## ğŸ” Privacy & Ethics
-All operations happen locally
-No user data is sent to the cloud.
-Open-source stack ensures transparency and auditability.

## ğŸ‘ Acknowledgements
-Qwen2.5 by Alibaba â€” for a powerful open-source LLM.
-Whisper by OpenAI â€” for transcription excellence.
-Streamlit â€” for making AI interfaces smooth and deployable.
-Ollama â€” for making local LLM hosting simple.


## ğŸ“¥ Supported File Uploads
-ğŸ§ Audio files â†’ Transcribed using Whisper
-ğŸ“¹ Video files â†’ Auto audio-extracted + transcribed
-ğŸ“„ PDFs â†’ Summarized
-ğŸŒ Google Drive files â†’ Paste public link

## ğŸ“Š Feedback System
Stores all feedback in app/feedback_storage.json
Includes:
--âœ… User message
--ğŸ¤– Assistant response
--ğŸŒŸ Rating (1â€“5)
-Auto-handles file creation + error recovery
-ğŸ“ˆ Analytics Page (optional, coming soon)

---
## ğŸ“¦ requirements.txt
```bash
streamlit
requests
pydub
whisper
PyPDF2
torch
torchaudio
torchvision
openai
google-auth
google-auth-oauthlib
google-api-python-client
```

---
### ğŸ§  Default Communication Language
-Default: English
-Support for Indic languages included (future toggle)
-Can be switched via system prompt updates

### ğŸ™Œ Credits
-Qwen2.5 by Alibaba (via Ollama)
-Whisper by OpenAI
-Streamlit for frontend
-You â€” for building open-source AI tools!

### ğŸ“ƒ License
This project is released under the MIT License.
```vbnet
Let me know if you'd like the `report.md` template as well â€” and Iâ€™ll generate that next.
```